{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PyPDF2 import PdfReader\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        pdf = PdfReader(f)\n",
    "        for page_num in range(len(pdf.pages)):\n",
    "            try:\n",
    "                text += pdf.pages[page_num].extract_text() or \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting text from page {page_num + 1} of {pdf_path}: {e}\")\n",
    "                continue\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract PDF metadata including character count and additional fields\n",
    "def extract_pdf_metadata(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            print(f\"Analyzing {pdf_path}\")\n",
    "            pdf = PdfReader(f)\n",
    "            number_of_pages = len(pdf.pages)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            char_count = len(text)\n",
    "            \n",
    "            # Extracting parts from the file name\n",
    "            file_name = os.path.basename(pdf_path)\n",
    "            parts = file_name.split('_')\n",
    "            index = parts[0]\n",
    "            company = parts[1]\n",
    "            year = parts[2].split('.')[0]\n",
    "            \n",
    "            return {\n",
    "                \"file_name\": file_name,\n",
    "                \"number_of_pages\": number_of_pages,\n",
    "                \"character_count\": char_count,\n",
    "                \"index\": index,\n",
    "                \"company\": company,\n",
    "                \"year\": year\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {pdf_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metadata from multiple PDFs with progress indication and error handling\n",
    "def extract_metadata_with_progress(pdf_files):\n",
    "    metadata = []\n",
    "    total_files = len(pdf_files)\n",
    "    for i, pdf in enumerate(pdf_files, start=1):\n",
    "        data = extract_pdf_metadata(pdf)\n",
    "        if data:\n",
    "            metadata.append(data)\n",
    "        print(f\"Pdf {i}/{total_files} analyzed\")\n",
    "    return pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to PDF files\n",
    "pdf_folder = \"../dataset_download/dataset\"\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata with progress indication\n",
    "df = extract_metadata_with_progress(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary statistics\n",
    "print(df.columns)\n",
    "df = pd.read_csv('company_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of number of pages\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['number_of_pages'], bins=30, kde=True)\n",
    "plt.title('Distribution of Number of Pages per Document')\n",
    "plt.xlabel('Number of Pages')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of document sizes\n",
    "file_sizes = [os.path.getsize(pdf) / 1024 for pdf in pdf_files]  # Sizes in KB\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(file_sizes, bins=30, kde=True)\n",
    "plt.title('Distribution of Document Sizes (KB)')\n",
    "plt.xlabel('Size (KB)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of character counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['character_count'], bins=30, kde=True)\n",
    "plt.title('Distribution of Character Count per Document')\n",
    "plt.xlabel('Character Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of different years and sort by the year\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(x=year_counts.index, y=year_counts.values)\n",
    "plt.title('Document Creation Dates Timeline')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adding the counts above the bars\n",
    "for index, value in enumerate(year_counts.values):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of different indices\n",
    "index_counts = df['index'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(x=index_counts.index, y=index_counts.values)\n",
    "plt.title('Count of Different Indices')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Adding the counts above the bars\n",
    "for index, value in enumerate(index_counts.values):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Count of different companies\n",
    "company_counts = df['company'].value_counts()\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(company_counts)\n",
    "\n",
    "# Plotting the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Company Names Word Cloud')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Extract filenames from the pdf_files list\n",
    "pdf_filenames = [os.path.basename(file) for file in pdf_files]\n",
    "\n",
    "# Extract filenames stored in the DataFrame\n",
    "df_filenames = df['file_name'].tolist()\n",
    "\n",
    "# Find the missing filenames\n",
    "missing_filenames = set(pdf_filenames) - set(df_filenames)\n",
    "\n",
    "# List the missing filenames\n",
    "missing_files = [file for file in pdf_files if os.path.basename(file) in missing_filenames]\n",
    "\n",
    "# Print the missing files\n",
    "for file in missing_files:\n",
    "    print(\"Missing files:\", (file))\n",
    "print(\"Number of missing files:\", len(missing_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract metadata for encrypted PDFs\n",
    "def extract_encrypted_pdf_metadata(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            pdf = PdfReader(f)\n",
    "            if pdf.is_encrypted:\n",
    "                try:\n",
    "                    pdf.decrypt(\"\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decrypting file {pdf_path}: {e}\")\n",
    "                    return None\n",
    "            number_of_pages = len(pdf.pages)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            char_count = len(text)\n",
    "            \n",
    "            # Extracting parts from the file name\n",
    "            file_name = os.path.basename(pdf_path)\n",
    "            parts = file_name.split('_')\n",
    "            index = parts[0]\n",
    "            company = parts[1]\n",
    "            year = parts[2].split('.')[0]\n",
    "            \n",
    "            return {\n",
    "                \"file_name\": file_name,\n",
    "                \"number_of_pages\": number_of_pages,\n",
    "                \"character_count\": char_count,\n",
    "                \"index\": index,\n",
    "                \"company\": company,\n",
    "                \"year\": year\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing encrypted file {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract metadata from multiple PDFs with progress indication and error handling\n",
    "def extract_metadata_with_progress2(pdf_files):\n",
    "    metadata = []\n",
    "    total_files = len(pdf_files)\n",
    "    for i, pdf in enumerate(pdf_files, start=1):\n",
    "        if \"ACIA_2018\" in pdf:\n",
    "            print(f\"Skipping corrupted file: {pdf}\")\n",
    "            continue\n",
    "        if \"ADBE\" in pdf:\n",
    "            data = extract_encrypted_pdf_metadata(pdf)\n",
    "        else:\n",
    "            data = extract_pdf_metadata(pdf)\n",
    "        if data:\n",
    "            metadata.append(data)\n",
    "        print(f\"Pdf {i}/{total_files} analyzed\")\n",
    "    return pd.DataFrame(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = extract_metadata_with_progress2(missing_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AWS credentials\n",
    "AWS_ACCESS_KEY=os.getenv('AWS_ACCESS_KEY')\n",
    "AWS_SECRET_KEY=os.getenv('AWS_SECRET_KEY')\n",
    "AWS_BUCKET_NAME = os.getenv('AWS_BUCKET_NAME')\n",
    "AWS_BUCKET_PREFIX = os.getenv('AWS_BUCKET_PREFIX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# Set the custom download directory - Change to desired directory\n",
    "DOWNLOAD_DIR = r'./pdfs'\n",
    "\n",
    "# Ensure the directory exists or create it\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize an S3 client using provided credentials\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY\n",
    ")\n",
    "\n",
    "# List of missing filenames\n",
    "missing_filenames = [\n",
    "    \"NASDAQ_ACIA_2018.pdf\",\n",
    "    \"NASDAQ_ADBE_2020.pdf\",\n",
    "    \"NASDAQ_ADBE_2021.pdf\",\n",
    "    \"NASDAQ_ADBE_2022.pdf\"\n",
    "]\n",
    "\n",
    "# List and download only the files in missing_filenames from the specified S3 bucket/prefix\n",
    "response = s3.list_objects_v2(Bucket=AWS_BUCKET_NAME, Prefix=AWS_BUCKET_PREFIX)\n",
    "for content in response.get('Contents', []):\n",
    "    file_key = content['Key']\n",
    "    file_name = file_key.split('/')[-1]\n",
    "    if file_name and file_name in missing_filenames:  # Check if file is in missing_filenames\n",
    "        local_path = os.path.join(DOWNLOAD_DIR, file_name)\n",
    "        print(f\"Downloading {file_name} to {local_path}...\")\n",
    "        s3.download_file(AWS_BUCKET_NAME, file_key, local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# Set the custom download directory - Change to desired directory\n",
    "DOWNLOAD_DIR = r'./pdfs'\n",
    "\n",
    "# Ensure the directory exists or create it\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize an S3 client using provided credentials\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY\n",
    ")\n",
    "\n",
    "# Prefix to filter files that start with \"NASDAQ_ACIA\"\n",
    "FILTER_PREFIX = \"NASDAQ_ACIA%20_2018\"\n",
    "\n",
    "# List and download only the files that start with FILTER_PREFIX from the specified S3 bucket/prefix\n",
    "response = s3.list_objects_v2(Bucket=AWS_BUCKET_NAME, Prefix=AWS_BUCKET_PREFIX)\n",
    "for content in response.get('Contents', []):\n",
    "    file_key = content['Key']\n",
    "    file_name = file_key.split('/')[-1]\n",
    "    if file_name and file_name.startswith(FILTER_PREFIX):  # Check if file name starts with FILTER_PREFIX\n",
    "        local_path = os.path.join(DOWNLOAD_DIR, file_name)\n",
    "        print(f\"Downloading {file_name} to {local_path}...\")\n",
    "        s3.download_file(AWS_BUCKET_NAME, file_key, local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to PDF files\n",
    "corrupted_files = \"C:\\\\Users\\\\Shaggz\\\\Desktop\\\\Programming2024\\\\ChatMultiplePDF\\\\Pdfs\"\n",
    "corrupted_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "\n",
    "df3 = extract_metadata_with_progress2(missing_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.company.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"character_count\"].sum()/800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"metadata.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"character_count\"].sum()/df[\"number_of_pages\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"character_count\"].sum()/800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.company.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# List of stock tickers\n",
    "tickers = df.company.unique()\n",
    "\n",
    "# Function to get company name from ticker\n",
    "def get_company_name(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        return stock.info['longName']\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "# Get company names for all tickers\n",
    "company_names = {ticker: get_company_name(ticker) for ticker in tickers}\n",
    "\n",
    "# Print the company names\n",
    "for ticker, name in company_names.items():\n",
    "    print(f\"{ticker}: {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
